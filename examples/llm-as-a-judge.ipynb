{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge Evaluation with Flex-Evals\n",
    "\n",
    "This notebook demonstrates how to use Large Language Models (LLMs) to evaluate AI system outputs using the Flexible Evaluation Protocol (FEP). LLM-as-a-judge is particularly powerful for evaluating subjective qualities like helpfulness, accuracy, tone, and coherence that are difficult to capture with traditional programmatic checks.\n",
    "\n",
    "## Why LLM-as-a-Judge?\n",
    "\n",
    "Traditional evaluation metrics work well for objective criteria (exact matches, thresholds, etc.), but many real-world AI applications need evaluation of subjective qualities:\n",
    "\n",
    "- **Helpfulness**: Is the response useful to the user?\n",
    "- **Accuracy**: Does the response contain correct information?\n",
    "- **Clarity**: Is the explanation easy to understand?\n",
    "- **Completeness**: Does the response fully address the question?\n",
    "- **Tone**: Is the response appropriate for the context?\n",
    "\n",
    "LLM-as-a-judge allows us to evaluate these nuanced criteria at scale while maintaining consistency and auditability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed: pip install flex-evals sik-llms pydantic\n",
    "#\n",
    "# IMPORTANT: This notebook requires valid LLM API credentials to run the judge function.\n",
    "# If you're seeing N/A values or errors, please ensure:\n",
    "# 1. You have API credentials configured (e.g., OPENAI_API_KEY environment variable)\n",
    "# 2. The sik-llms package is properly configured with your LLM provider\n",
    "# 3. Your API key has sufficient quota/credits\n",
    "#\n",
    "# The examples will show the structure and flow even without API access, but actual\n",
    "# LLM judge evaluations require a working LLM connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from pydantic import BaseModel, Field\n",
    "from sik_llms import create_client, system_message, user_message\n",
    "\n",
    "from flex_evals import evaluate\n",
    "from flex_evals.schemas import TestCase, Output\n",
    "from flex_evals import LLMJudgeCheck\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # for running async function in a Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Evaluation Criteria\n",
    "\n",
    "We use Pydantic models to define the structure of our evaluation results. This ensures type safety and provides clear documentation of what the LLM judge should evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryEvaluation(BaseModel):\n",
    "    \"\"\"Simple yes/no evaluation with reasoning.\"\"\"\n",
    "\n",
    "    answers_question: bool = Field(description=\"Whether the response answers the user's question\")\n",
    "    reasoning: str = Field(description=\"Brief explanation of the evaluation\")\n",
    "\n",
    "\n",
    "class DetailedQualityAssessment(BaseModel):\n",
    "    \"\"\"Comprehensive quality evaluation with multiple criteria.\"\"\"\n",
    "\n",
    "    overall_score: int = Field(ge=1, le=5, description=\"Overall quality score from 1 (poor) to 5 (excellent)\")  # noqa: E501\n",
    "    helpfulness: int = Field(ge=1, le=5, description=\"How helpful is the response to the user?\")\n",
    "    accuracy: int = Field(ge=1, le=5, description=\"How accurate is the information provided?\")\n",
    "    clarity: int = Field(ge=1, le=5, description=\"How clear and easy to understand is the response?\")  # noqa: E501\n",
    "    completeness: int = Field(ge=1, le=5, description=\"How completely does the response address the question?\")  # noqa: E501\n",
    "    strengths: list[str] = Field(description=\"Key strengths of the response\")\n",
    "    weaknesses: list[str] = Field(description=\"Areas for improvement\")\n",
    "    recommendation: str = Field(description=\"Overall recommendation for this response quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create LLM Judge Function\n",
    "\n",
    "We create a function that uses the sik_llms framework to call an LLM for evaluation. This function will be passed to our LLM judge check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_llm_judge_function(model_name: str = 'gpt-4o-mini') -> callable:\n",
    "    \"\"\"Create an LLM judge function using sik_llms framework.\"\"\"\n",
    "\n",
    "    async def llm_judge(prompt: str, response_format: type[BaseModel]) -> tuple[BaseModel, dict[str, Any]]:  # noqa: E501\n",
    "        \"\"\"\n",
    "        LLM judge function that evaluates based on the given prompt.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (parsed_response, metadata_dict)\n",
    "        \"\"\"\n",
    "        # Create client with the specified response format\n",
    "        client = create_client(\n",
    "            model_name=model_name,\n",
    "            response_format=response_format,\n",
    "        )\n",
    "\n",
    "        # Create messages for the evaluation\n",
    "        messages = [\n",
    "            system_message(\n",
    "                \"You are an expert evaluator tasked with assessing AI responses. \"\n",
    "                \"Provide objective, fair, and constructive evaluations based on the given criteria. \"  # noqa: E501\n",
    "                \"Be specific in your reasoning and provide actionable feedback.\",\n",
    "            ),\n",
    "            user_message(prompt),\n",
    "        ]\n",
    "\n",
    "        # Get evaluation from LLM\n",
    "        response = await client.run_async(messages=messages)\n",
    "\n",
    "        # Extract metadata from the response\n",
    "        metadata = {\n",
    "            \"cost_usd\": response.input_cost + response.output_cost,\n",
    "            \"tokens_used\": response.input_tokens + response.output_tokens,\n",
    "            \"input_tokens\": response.input_tokens,\n",
    "            \"output_tokens\": response.output_tokens,\n",
    "            \"response_time_ms\": int(response.duration_seconds * 1000),\n",
    "            \"model_version\": model_name,\n",
    "        }\n",
    "\n",
    "        return response.parsed, metadata\n",
    "\n",
    "    return llm_judge\n",
    "\n",
    "\n",
    "# Create our judge function\n",
    "llm_judge_function = await create_llm_judge_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Simple Example - Does the AI Answer the Question?\n",
    "\n",
    "Let's start with a simple binary evaluation: does the AI response actually answer the user's question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the llm-as-a-judge check we want to perform on the AI responses\n",
    "# Since we are using JSONPath to access test case input and expected values,\n",
    "# we can reuse the same check for multiple test cases\n",
    "llm_judge_check = LLMJudgeCheck(\n",
    "    # Note that we use JSONPath to access test case input and expected values\n",
    "    prompt=\"\"\"\n",
    "        Evaluate whether the AI response properly answers the user's question.\n",
    "\n",
    "        **User Question:** {{$.test_case.input}}\n",
    "        **AI Response:** {{$.output.value}}\n",
    "        **Expected Answer:** {{$.test_case.expected}}\n",
    "\n",
    "        Consider:\n",
    "        - Does the response directly address the question?\n",
    "        - Is the core information correct?\n",
    "        - Is the response complete enough to be useful?\n",
    "        \"\"\",\n",
    "    response_format=BinaryEvaluation,\n",
    "    llm_function=llm_judge_function,\n",
    ")\n",
    "\n",
    "# Create test cases for question-answering evaluation\n",
    "simple_test_cases = [\n",
    "    TestCase(\n",
    "        id=\"qa_001\",\n",
    "        input=\"What is the capital of France?\",\n",
    "        expected=\"Paris\",  # We know the correct answer\n",
    "        metadata={\"category\": \"geography\", \"difficulty\": \"easy\"},\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"qa_002\",\n",
    "        input=\"How do I bake a chocolate cake?\",\n",
    "        expected=\"Step-by-step baking instructions\",\n",
    "        metadata={\"category\": \"cooking\", \"difficulty\": \"medium\"},\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"qa_003\",\n",
    "        input=\"What's the weather like?\",\n",
    "        expected=\"Request for location or explanation of inability to provide weather\",\n",
    "        metadata={\"category\": \"general\", \"difficulty\": \"easy\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Simulated AI responses (in practice, these would come from your AI system)\n",
    "simple_outputs = [\n",
    "    Output(\n",
    "        value=\"The capital of France is Paris.\",\n",
    "        metadata={\"model\": \"gpt-4\", \"confidence\": 0.99},\n",
    "    ),\n",
    "    Output(\n",
    "        value=\"To bake a chocolate cake: 1) Preheat oven to 350Â°F, 2) Mix dry ingredients, 3) Add wet ingredients, 4) Bake for 30-35 minutes. You'll need flour, sugar, cocoa powder, eggs, butter, and baking powder.\",  # noqa: E501\n",
    "        metadata={\"model\": \"gpt-4\", \"confidence\": 0.95},\n",
    "    ),\n",
    "    Output(\n",
    "        value=\"I can't provide current weather information as I don't have access to real-time data. Could you please specify your location and I can suggest ways to check the current weather?\",  # noqa: E501\n",
    "        metadata={\"model\": \"gpt-4\", \"confidence\": 0.90},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ð Running Simple Question-Answering Evaluation...\n",
      "\n",
      "ð Evaluation Results:\n",
      "   Total test cases: 3\n",
      "   Completed: 3\n",
      "   Errors: 0\n",
      "   Overall status: completed\n",
      "\n",
      "ð§ª Test Case: qa_001 (geography)\n",
      "   Question: What is the capital of France?\n",
      "   AI Response: The capital of France is Paris.\n",
      "   â Answers Question: True\n",
      "   ð­ Reasoning: The AI response accurately answers the user's question by providing the correct capital of France, which is Paris. The information is direct and complete.\n",
      "   ð Judge Metadata:\n",
      "      - Cost: $0.0001\n",
      "      - Tokens: 234\n",
      "      - Response Time: 1195ms\n",
      "      - Model: gpt-4o-mini\n",
      "\n",
      "ð§ª Test Case: qa_002 (cooking)\n",
      "   Question: How do I bake a chocolate cake?\n",
      "   AI Response: To bake a chocolate cake: 1) Preheat oven to 350Â°F, 2) Mix dry ingredients, 3) Add wet ingredients, ...\n",
      "   â Answers Question: True\n",
      "   ð­ Reasoning: The AI response directly answers the user's question by providing a general step-by-step guide to baking a chocolate cake. The core information about ingredients and the baking process is correct. However, the response could be improved by providing more detailed instructions on mixing and baking, such as mixing times or specific baking methods.\n",
      "   ð Judge Metadata:\n",
      "      - Cost: $0.0001\n",
      "      - Tokens: 324\n",
      "      - Response Time: 2340ms\n",
      "      - Model: gpt-4o-mini\n",
      "\n",
      "ð§ª Test Case: qa_003 (general)\n",
      "   Question: What's the weather like?\n",
      "   AI Response: I can't provide current weather information as I don't have access to real-time data. Could you plea...\n",
      "   â Answers Question: True\n",
      "   ð­ Reasoning: The AI response acknowledges the inability to provide real-time weather data and correctly prompts the user to specify their location for further assistance. This directly addresses the user's request for weather information in a practical manner.\n",
      "   ð Judge Metadata:\n",
      "      - Cost: $0.0001\n",
      "      - Tokens: 279\n",
      "      - Response Time: 1611ms\n",
      "      - Model: gpt-4o-mini\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the simple evaluation\n",
    "print(\"ð Running Simple Question-Answering Evaluation...\\n\")\n",
    "\n",
    "simple_results = evaluate(\n",
    "    test_cases=simple_test_cases,\n",
    "    outputs=simple_outputs,\n",
    "    checks=[llm_judge_check],\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"ð Evaluation Results:\")\n",
    "print(f\"   Total test cases: {simple_results.summary.total_test_cases}\")\n",
    "print(f\"   Completed: {simple_results.summary.completed_test_cases}\")\n",
    "print(f\"   Errors: {simple_results.summary.error_test_cases}\")\n",
    "print(f\"   Overall status: {simple_results.status}\\n\")\n",
    "\n",
    "# Show detailed results for each test case\n",
    "for i, result in enumerate(simple_results.results):\n",
    "    test_case = simple_test_cases[i]\n",
    "    check_result = result.check_results[0]\n",
    "\n",
    "    # The response fields are now directly in results\n",
    "    results = check_result.results\n",
    "    metadata = results.get('judge_metadata', {})\n",
    "\n",
    "    print(f\"ð§ª Test Case: {test_case.id} ({test_case.metadata['category']})\")\n",
    "    print(f\"   Question: {test_case.input}\")\n",
    "    print(f\"   AI Response: {simple_outputs[i].value[:100]}{'...' if len(simple_outputs[i].value) > 100 else ''}\")  # noqa: E501\n",
    "    print(f\"   â Answers Question: {results.get('answers_question', 'N/A')}\")\n",
    "    print(f\"   ð­ Reasoning: {results.get('reasoning', 'N/A')}\")\n",
    "\n",
    "    # Display metadata if available\n",
    "    if metadata:\n",
    "        print(\"   ð Judge Metadata:\")\n",
    "        print(f\"      - Cost: ${metadata.get('cost_usd', 0):.4f}\")\n",
    "        print(f\"      - Tokens: {metadata.get('tokens_used', 0)}\")\n",
    "        print(f\"      - Response Time: {metadata.get('response_time_ms', 0)}ms\")\n",
    "        print(f\"      - Model: {metadata.get('model_version', 'unknown')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Complex Example - Multi-Criteria Quality Assessment\n",
    "\n",
    "Now let's do a more sophisticated evaluation that assesses multiple quality dimensions and provides detailed feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test cases for detailed quality assessment\n",
    "# Unlike the previous example, each test case has it's own llm-as-a-judge check\n",
    "complex_test_cases = [\n",
    "    TestCase(\n",
    "        id=\"quality_001\",\n",
    "        input={\n",
    "            \"question\": \"Can you explain how machine learning works?\",\n",
    "            \"user_context\": \"I'm a beginner with no technical background\",\n",
    "            \"desired_length\": \"medium explanation\",\n",
    "        },\n",
    "        expected={\n",
    "            \"key_concepts\": [\"training\", \"data\", \"patterns\", \"predictions\"],\n",
    "            \"complexity_level\": \"beginner-friendly\",\n",
    "            \"includes_examples\": True,\n",
    "        },\n",
    "        metadata={\"domain\": \"AI/ML\", \"audience\": \"beginner\"},\n",
    "        checks=[\n",
    "            LLMJudgeCheck(\n",
    "                prompt=\"\"\"\n",
    "                Provide a comprehensive quality assessment of this AI response.\n",
    "\n",
    "                **Context:**\n",
    "                - User Question: {{$.test_case.input.question}}\n",
    "                - User Background: {{$.test_case.input.user_context}}\n",
    "                - Desired Length: {{$.test_case.input.desired_length}}\n",
    "                - Target Audience: {{$.test_case.metadata.audience}}\n",
    "\n",
    "                **AI Response to Evaluate:**\n",
    "                {{$.output.value}}\n",
    "\n",
    "                **Expected Elements:**\n",
    "                - Key concepts to cover: {{$.test_case.expected.key_concepts}}\n",
    "                - Complexity level: {{$.test_case.expected.complexity_level}}\n",
    "                - Should include examples: {{$.test_case.expected.includes_examples}}\n",
    "\n",
    "                **Evaluation Criteria:**\n",
    "\n",
    "                1. **Helpfulness (1-5)**: How useful is this response to someone with the user's background?\n",
    "                2. **Accuracy (1-5)**: Is the technical information correct and up-to-date?\n",
    "                3. **Clarity (1-5)**: Is the explanation clear and easy to understand for the target audience?\n",
    "                4. **Completeness (1-5)**: Does it adequately cover the topic without being overwhelming?\n",
    "\n",
    "                Provide specific feedback on strengths and areas for improvement.\n",
    "                \"\"\",  # noqa: E501\n",
    "                response_format=DetailedQualityAssessment,\n",
    "                llm_function=llm_judge_function,\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"quality_002\",\n",
    "        input={\n",
    "            \"question\": \"How do I fix my code that keeps crashing?\",\n",
    "            \"user_context\": \"Intermediate programmer, Python development\",\n",
    "            \"desired_length\": \"concise but complete\",\n",
    "        },\n",
    "        expected={\n",
    "            \"key_concepts\": [\"debugging\", \"error messages\", \"troubleshooting steps\"],\n",
    "            \"complexity_level\": \"intermediate\",\n",
    "            \"includes_examples\": True,\n",
    "        },\n",
    "        metadata={\"domain\": \"programming\", \"audience\": \"intermediate\"},\n",
    "        checks=[\n",
    "            LLMJudgeCheck(\n",
    "                prompt=\"\"\"\n",
    "                Evaluate this programming help response for technical accuracy and usefulness.\n",
    "\n",
    "                **Context:**\n",
    "                - User Question: {{$.test_case.input.question}}\n",
    "                - User Background: {{$.test_case.input.user_context}}\n",
    "                - Desired Style: {{$.test_case.input.desired_length}}\n",
    "                - Domain: {{$.test_case.metadata.domain}}\n",
    "\n",
    "                **AI Response to Evaluate:**\n",
    "                {{$.output.value}}\n",
    "\n",
    "                **Expected Elements:**\n",
    "                - Should cover: {{$.test_case.expected.key_concepts}}\n",
    "                - Complexity: {{$.test_case.expected.complexity_level}}\n",
    "                - Examples needed: {{$.test_case.expected.includes_examples}}\n",
    "\n",
    "                Focus on whether the response provides actionable debugging advice appropriate for an intermediate programmer.\n",
    "                \"\"\",  # noqa: E501\n",
    "                response_format=DetailedQualityAssessment,\n",
    "                llm_function=llm_judge_function,\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Simulated AI responses for complex evaluation\n",
    "complex_outputs = [\n",
    "    Output(\n",
    "        value=\"\"\"Machine learning is like teaching a computer to recognize patterns, similar to how you might learn to recognize your friends' faces.\n",
    "\n",
    "Here's how it works in simple terms:\n",
    "\n",
    "1. **Training with Data**: First, we show the computer lots of examples. For instance, if we want it to recognize cats, we show it thousands of cat photos labeled \"cat.\"\n",
    "\n",
    "2. **Finding Patterns**: The computer looks for common features in all those cat photos - things like pointy ears, whiskers, or fur patterns.\n",
    "\n",
    "3. **Making Predictions**: When you show it a new photo, it uses those patterns to guess whether it's a cat or not.\n",
    "\n",
    "Think of it like learning to cook - the more recipes you follow (training data), the better you get at creating new dishes (making predictions). The computer gets \"smarter\" by seeing more examples, just like you get better at cooking with practice!\n",
    "\n",
    "This same process helps computers do amazing things like recommend movies on Netflix, translate languages, or even help doctors diagnose diseases.\"\"\",  # noqa: E501\n",
    "        metadata={\"model\": \"gpt-4\", \"response_time_ms\": 1200, \"confidence\": 0.92},\n",
    "    ),\n",
    "    Output(\n",
    "        value=\"\"\"Here's a systematic approach to debug crashing Python code:\n",
    "\n",
    "1. **Read the error message carefully** - Python error messages usually tell you exactly what's wrong and where.\n",
    "\n",
    "2. **Check the stack trace** - Look at the last few lines to see which line of code caused the crash.\n",
    "\n",
    "3. **Common crash causes**:\n",
    "   - `NameError`: Variable not defined\n",
    "   - `IndexError`: Accessing list element that doesn't exist\n",
    "   - `KeyError`: Dictionary key not found\n",
    "   - `TypeError`: Wrong data type for operation\n",
    "\n",
    "4. **Debugging techniques**:\n",
    "   ```python\n",
    "   # Add print statements to trace execution\n",
    "   print(f\"Variable x = {x}\")\n",
    "\n",
    "   # Use try-except to catch specific errors\n",
    "   try:\n",
    "       risky_operation()\n",
    "   except IndexError as e:\n",
    "       print(f\"Index error: {e}\")\n",
    "   ```\n",
    "\n",
    "5. **Use a debugger** - Try `pdb.set_trace()` or your IDE's debugger to step through code line by line.\n",
    "\n",
    "If you share the specific error message, I can give more targeted advice!\"\"\",  # noqa: E501\n",
    "        metadata={\"model\": \"gpt-4\", \"response_time_ms\": 800, \"confidence\": 0.88},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ð Running Detailed Quality Assessment...\n",
      "\n",
      "ð Evaluation Results:\n",
      "   Total test cases: 2\n",
      "   Completed: 2\n",
      "   Errors: 0\n",
      "   Overall status: completed\n",
      "\n",
      "ð§ª Test Case: quality_001 (AI/ML)\n",
      "   Question: Can you explain how machine learning works?\n",
      "   User Context: I'm a beginner with no technical background\n",
      "\n",
      "   ð **Quality Scores:**\n",
      "      Overall: 4/5\n",
      "      Helpfulness: 5/5\n",
      "      Accuracy: 4/5\n",
      "      Clarity: 5/5\n",
      "      Completeness: 4/5\n",
      "\n",
      "   â **Strengths:**\n",
      "      â¢ The response is beginner-friendly and uses relatable analogies to explain a complex topic.\n",
      "      â¢ It clearly defines key concepts like training, data, patterns, and predictions in an accessible manner.\n",
      "      â¢ The cooking analogy is effective and makes the learning process relatable.\n",
      "\n",
      "   â ï¸  **Areas for Improvement:**\n",
      "      â¢ While the explanation covers major points succinctly, it could involve a more structured discussion of how these concepts work together in specific instances of machine learning.\n",
      "      â¢ Additional examples or applications could enhance the comprehensiveness of the answer.\n",
      "\n",
      "   ð¡ **Recommendation:** Great overall response with strong clarity and accessibility. To improve further, consider adding more structured detail on how the key concepts interconnect and include a few more examples to illustrate machine learning applications.\n",
      "\n",
      "   ð **Judge Metadata:**\n",
      "      - Cost: $0.0002\n",
      "      - Tokens: 882 (in: 716, out: 166)\n",
      "      - Response Time: 2372ms\n",
      "      - Model: gpt-4o-mini\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ð§ª Test Case: quality_002 (programming)\n",
      "   Question: How do I fix my code that keeps crashing?\n",
      "   User Context: Intermediate programmer, Python development\n",
      "\n",
      "   ð **Quality Scores:**\n",
      "      Overall: 4/5\n",
      "      Helpfulness: 5/5\n",
      "      Accuracy: 5/5\n",
      "      Clarity: 4/5\n",
      "      Completeness: 4/5\n",
      "\n",
      "   â **Strengths:**\n",
      "      â¢ Provides a systematic approach to debugging that is easy to follow.\n",
      "      â¢ Lists common crash causes with clear error types and descriptions, which are relevant for an intermediate programmer.\n",
      "      â¢ Includes practical examples like print statements, try-except blocks, and using the debugger.\n",
      "\n",
      "   â ï¸  **Areas for Improvement:**\n",
      "      â¢ While the response is complete, it could have included at least one more detailed example or a specific scenario to illustrate the debugging techniques.\n",
      "      â¢ Clarification on how to interpret stack traces could enhance understanding for the user.\n",
      "\n",
      "   ð¡ **Recommendation:** This response is highly useful and accurate for an intermediate programmer. Consider adding more examples or detailed guidance on interpreting stack traces for improved clarity.\n",
      "\n",
      "   ð **Judge Metadata:**\n",
      "      - Cost: $0.0002\n",
      "      - Tokens: 788 (in: 633, out: 155)\n",
      "      - Response Time: 2559ms\n",
      "      - Model: gpt-4o-mini\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the complex evaluation\n",
    "print(\"ð Running Detailed Quality Assessment...\\n\")\n",
    "\n",
    "complex_results = evaluate(complex_test_cases, complex_outputs)\n",
    "\n",
    "# Display results\n",
    "print(\"ð Evaluation Results:\")\n",
    "print(f\"   Total test cases: {complex_results.summary.total_test_cases}\")\n",
    "print(f\"   Completed: {complex_results.summary.completed_test_cases}\")\n",
    "print(f\"   Errors: {complex_results.summary.error_test_cases}\")\n",
    "print(f\"   Overall status: {complex_results.status}\\n\")\n",
    "\n",
    "# Show detailed results for each test case\n",
    "for i, result in enumerate(complex_results.results):\n",
    "    test_case = complex_test_cases[i]\n",
    "    check_result = result.check_results[0]\n",
    "\n",
    "    # Check for errors first\n",
    "    if check_result.error:\n",
    "        print(f\"â ERROR in Test Case: {test_case.id}\")\n",
    "        print(f\"   Error Type: {check_result.error.type}\")\n",
    "        print(f\"   Error Message: {check_result.error.message}\")\n",
    "        print(f\"   Recoverable: {check_result.error.recoverable}\")\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        continue\n",
    "\n",
    "    # The response fields are now directly in results\n",
    "    results = check_result.results\n",
    "    metadata = results.get('judge_metadata', {})\n",
    "\n",
    "    print(f\"ð§ª Test Case: {test_case.id} ({test_case.metadata['domain']})\")\n",
    "    print(f\"   Question: {test_case.input['question']}\")\n",
    "    print(f\"   User Context: {test_case.input['user_context']}\")\n",
    "    print()\n",
    "    print(\"   ð **Quality Scores:**\")\n",
    "    print(f\"      Overall: {results.get('overall_score', 'N/A')}/5\")\n",
    "    print(f\"      Helpfulness: {results.get('helpfulness', 'N/A')}/5\")\n",
    "    print(f\"      Accuracy: {results.get('accuracy', 'N/A')}/5\")\n",
    "    print(f\"      Clarity: {results.get('clarity', 'N/A')}/5\")\n",
    "    print(f\"      Completeness: {results.get('completeness', 'N/A')}/5\")\n",
    "    print()\n",
    "    print(\"   â **Strengths:**\")\n",
    "    for strength in results.get('strengths', []):\n",
    "        print(f\"      â¢ {strength}\")\n",
    "    print()\n",
    "    print(\"   â ï¸  **Areas for Improvement:**\")\n",
    "    for weakness in results.get('weaknesses', []):\n",
    "        print(f\"      â¢ {weakness}\")\n",
    "    print()\n",
    "    print(f\"   ð¡ **Recommendation:** {results.get('recommendation', 'N/A')}\")\n",
    "\n",
    "    # Display metadata if available\n",
    "    if metadata:\n",
    "        print()\n",
    "        print(\"   ð **Judge Metadata:**\")\n",
    "        print(f\"      - Cost: ${metadata.get('cost_usd', 0):.4f}\")\n",
    "        print(f\"      - Tokens: {metadata.get('tokens_used', 0)} (in: {metadata.get('input_tokens', 0)}, out: {metadata.get('output_tokens', 0)})\")  # noqa: E501\n",
    "        print(f\"      - Response Time: {metadata.get('response_time_ms', 0)}ms\")\n",
    "        print(f\"      - Model: {metadata.get('model_version', 'unknown')}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Batch Evaluation Example\n",
    "\n",
    "Let's demonstrate how to evaluate multiple AI responses efficiently, which is useful for:\n",
    "- A/B testing different models\n",
    "- Evaluating prompt engineering changes\n",
    "- Quality assurance on production responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of customer service responses to evaluate\n",
    "customer_service_cases = [\n",
    "    TestCase(\n",
    "        id=\"cs_001\",\n",
    "        input=\"I'm really frustrated! My order hasn't arrived and it's been a week. What's going on?\",  # noqa: E501\n",
    "        metadata={\"sentiment\": \"angry\", \"issue_type\": \"delivery_delay\"},\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"cs_002\",\n",
    "        input=\"Hi, I need to return this item. It doesn't fit properly. What's your return policy?\",  # noqa: E501\n",
    "        metadata={\"sentiment\": \"neutral\", \"issue_type\": \"return_request\"},\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"cs_003\",\n",
    "        input=\"Your product is amazing! Just wanted to say thanks. Also, do you have any similar products?\",  # noqa: E501\n",
    "        metadata={\"sentiment\": \"positive\", \"issue_type\": \"product_inquiry\"},\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"cs_004\",\n",
    "        input=\"I was charged twice for the same order. This is unacceptable. I want a refund immediately.\",  # noqa: E501\n",
    "        metadata={\"sentiment\": \"very_angry\", \"issue_type\": \"billing_error\"},\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"cs_005\",\n",
    "        input=\"Quick question - what are your business hours? I want to call later.\",\n",
    "        metadata={\"sentiment\": \"neutral\", \"issue_type\": \"information_request\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Use the same evaluation check to all test cases\n",
    "customer_service_check = LLMJudgeCheck(\n",
    "    prompt=\"\"\"\n",
    "    Evaluate this customer service response for quality and appropriateness.\n",
    "\n",
    "    **Customer Message:** {{$.test_case.input}}\n",
    "    **Customer Sentiment:** {{$.test_case.metadata.sentiment}}\n",
    "    **Issue Type:** {{$.test_case.metadata.issue_type}}\n",
    "\n",
    "    **AI Response:** {{$.output.value}}\n",
    "\n",
    "    **Evaluation Criteria:**\n",
    "    - **Helpfulness**: Does the response address the customer's needs?\n",
    "    - **Empathy**: Is the tone appropriate for the customer's emotional state?\n",
    "    - **Professionalism**: Is the response professional and courteous?\n",
    "    - **Actionability**: Does it provide clear next steps or solutions?\n",
    "\n",
    "    Rate each dimension and provide an overall assessment.\n",
    "    \"\"\",\n",
    "    response_format=DetailedQualityAssessment,\n",
    "    llm_function=llm_judge_function,\n",
    ")\n",
    "\n",
    "# Simulated customer service AI responses\n",
    "customer_service_outputs = [\n",
    "    Output(value=\"I sincerely apologize for the delay with your order. I understand how frustrating this must be. Let me look into this immediately and provide you with a tracking update and expedited shipping at no cost.\"),  # noqa: E501\n",
    "    Output(value=\"Of course! Our return policy allows returns within 30 days. Since the fit isn't right, you can return it for a full refund or exchange. I'll email you a prepaid return label right now.\"),  # noqa: E501\n",
    "    Output(value=\"Thank you so much for the kind words! We're thrilled you love the product. Based on what you purchased, I think you'd really like our Premium Series - I'll send you some recommendations!\"),  # noqa: E501\n",
    "    Output(value=\"I apologize for this billing error - that's definitely not acceptable. I've immediately processed a refund for the duplicate charge, and you should see it in 2-3 business days. I've also added a credit to your account.\"),  # noqa: E501\n",
    "    Output(value=\"Our customer service hours are Monday-Friday 8 AM to 8 PM EST, and Saturday-Sunday 10 AM to 6 PM EST. You can also reach us anytime through our 24/7 chat support!\"),  # noqa: E501\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ð Running Customer Service Quality Batch Evaluation...\n",
      "\n",
      "ð **Customer Service Response Quality Report**\n",
      "\n",
      "| Test ID | Sentiment | Issue Type | Overall | Help | Accuracy | Clarity | Complete |\n",
      "|---------|-----------|------------|---------|------|----------|---------|----------|\n",
      "| cs_001 | angry | delivery_d | 5/5 | 5/5 | 5/5 | 5/5 | 5/5 |\n",
      "| cs_002 | neutral | return_req | 5/5 | 5/5 | 5/5 | 5/5 | 5/5 |\n",
      "| cs_003 | positive | product_in | 5/5 | 5/5 | 5/5 | 5/5 | 5/5 |\n",
      "| cs_004 | very_ang | billing_er | 4/5 | 5/5 | 5/5 | 4/5 | 4/5 |\n",
      "| cs_005 | neutral | informatio | 5/5 | 5/5 | 5/5 | 5/5 | 5/5 |\n",
      "\n",
      "ð **Average Scores Across All Responses:**\n",
      "   Overall: 4.80/5\n",
      "   Helpfulness: 5.00/5\n",
      "   Accuracy: 5.00/5\n",
      "   Clarity: 4.80/5\n",
      "   Completeness: 4.80/5\n",
      "\n",
      "ð **Best Response:** cs_001 (Score: 5/5)\n",
      "â ï¸  **Needs Improvement:** cs_004 (Score: 4/5)\n",
      "\n",
      "ð° **Total Evaluation Cost:** $0.0007\n",
      "ð¢ **Total Tokens Used:** 2,835\n",
      "ð **Average Cost per Evaluation:** $0.0001\n"
     ]
    }
   ],
   "source": [
    "# Run batch evaluation\n",
    "print(\"ð Running Customer Service Quality Batch Evaluation...\\n\")\n",
    "\n",
    "batch_results = evaluate(\n",
    "    test_cases=customer_service_cases,\n",
    "    outputs=customer_service_outputs,\n",
    "    checks=[customer_service_check],\n",
    ")\n",
    "\n",
    "# Calculate average scores across all responses\n",
    "all_scores = {\n",
    "    'overall': [],\n",
    "    'helpfulness': [],\n",
    "    'accuracy': [],\n",
    "    'clarity': [],\n",
    "    'completeness': [],\n",
    "}\n",
    "\n",
    "# Track total costs and tokens\n",
    "total_cost = 0.0\n",
    "total_tokens = 0\n",
    "\n",
    "print(\"ð **Customer Service Response Quality Report**\\n\")\n",
    "print(\"| Test ID | Sentiment | Issue Type | Overall | Help | Accuracy | Clarity | Complete |\")\n",
    "print(\"|---------|-----------|------------|---------|------|----------|---------|----------|\")\n",
    "\n",
    "for i, result in enumerate(batch_results.results):\n",
    "    test_case = customer_service_cases[i]\n",
    "\n",
    "    # Check for errors first\n",
    "    if result.check_results[0].error:\n",
    "        print(f\"| {test_case.id} | {test_case.metadata['sentiment'][:8]} | {test_case.metadata['issue_type'][:10]} | ERROR | ERROR | ERROR | ERROR | ERROR |\")  # noqa: E501\n",
    "        print(f\"Error: {result.check_results[0].error.message}\")\n",
    "        continue\n",
    "\n",
    "    # The response fields are now directly in results\n",
    "    results = result.check_results[0].results\n",
    "    metadata = results.get('judge_metadata', {})\n",
    "\n",
    "    # Track metadata\n",
    "    if metadata:\n",
    "        total_cost += metadata.get('cost_usd', 0)\n",
    "        total_tokens += metadata.get('tokens_used', 0)\n",
    "\n",
    "    # Collect scores for averaging\n",
    "    all_scores['overall'].append(results.get('overall_score', 0))\n",
    "    all_scores['helpfulness'].append(results.get('helpfulness', 0))\n",
    "    all_scores['accuracy'].append(results.get('accuracy', 0))\n",
    "    all_scores['clarity'].append(results.get('clarity', 0))\n",
    "    all_scores['completeness'].append(results.get('completeness', 0))\n",
    "\n",
    "    # Display in table format\n",
    "    print(f\"| {test_case.id} | {test_case.metadata['sentiment'][:8]} | {test_case.metadata['issue_type'][:10]} | \"  # noqa: E501\n",
    "          f\"{results.get('overall_score', 'N/A')}/5 | {results.get('helpfulness', 'N/A')}/5 | {results.get('accuracy', 'N/A')}/5 | \"  # noqa: E501\n",
    "          f\"{results.get('clarity', 'N/A')}/5 | {results.get('completeness', 'N/A')}/5 |\")\n",
    "\n",
    "# Calculate and display averages\n",
    "print(\"\\nð **Average Scores Across All Responses:**\")\n",
    "for metric, scores in all_scores.items():\n",
    "    avg_score = sum(scores) / len(scores) if scores else 0\n",
    "    print(f\"   {metric.title()}: {avg_score:.2f}/5\")\n",
    "\n",
    "# Find best and worst performing responses\n",
    "overall_scores = all_scores['overall']\n",
    "if overall_scores and any(s > 0 for s in overall_scores):\n",
    "    best_idx = overall_scores.index(max(overall_scores))\n",
    "    worst_idx = overall_scores.index(min(overall_scores))\n",
    "\n",
    "    print(f\"\\nð **Best Response:** {customer_service_cases[best_idx].id} (Score: {overall_scores[best_idx]}/5)\")  # noqa: E501\n",
    "    print(f\"â ï¸  **Needs Improvement:** {customer_service_cases[worst_idx].id} (Score: {overall_scores[worst_idx]}/5)\")  # noqa: E501\n",
    "else:\n",
    "    print(\"\\nâ ï¸ No valid scores available - all evaluations may have failed\")\n",
    "\n",
    "# Display total metadata\n",
    "print(f\"\\nð° **Total Evaluation Cost:** ${total_cost:.4f}\")\n",
    "print(f\"ð¢ **Total Tokens Used:** {total_tokens:,}\")\n",
    "if len(batch_results.results) > 0:\n",
    "    print(f\"ð **Average Cost per Evaluation:** ${total_cost/len(batch_results.results):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Advanced Features\n",
    "\n",
    "### Template Processing Power\n",
    "\n",
    "Notice how we used `{{$.jsonpath}}` expressions in our prompts! This powerful feature allows dynamic prompt generation:\n",
    "\n",
    "- `{{$.test_case.input}}` - Access the test input\n",
    "- `{{$.output.value}}` - Access the AI response\n",
    "- `{{$.test_case.metadata.sentiment}}` - Access nested metadata\n",
    "- `{{$.output.metadata.confidence}}` - Access response metadata\n",
    "\n",
    "### Error Handling and Robustness\n",
    "\n",
    "The framework automatically handles:\n",
    "- Invalid JSONPath expressions\n",
    "- LLM API failures\n",
    "- Response format validation errors\n",
    "- Network timeouts and retries\n",
    "\n",
    "### Extensibility\n",
    "\n",
    "You can easily extend this for your use cases:\n",
    "- Custom response formats for domain-specific evaluation\n",
    "- Different LLM models for different types of evaluation\n",
    "- Integration with your existing evaluation pipelines\n",
    "- Automated report generation and alerting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leveraging Judge Metadata\n",
    "\n",
    "The new interface returns both the judge's response and metadata about the LLM call. This is incredibly useful for:\n",
    "\n",
    "1. **Cost Tracking**: Monitor and budget your LLM evaluation costs\n",
    "2. **Performance Monitoring**: Track response times and optimize slow evaluations  \n",
    "3. **Token Usage**: Understand token consumption for different types of evaluations\n",
    "4. **Model Versioning**: Track which model versions were used for evaluations\n",
    "\n",
    "### Example: Creating a Cost-Aware Judge Function\n",
    "\n",
    "Here's how you can create an LLM judge function that returns comprehensive metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Custom LLM judge function with detailed metadata\n",
    "async def create_custom_judge_with_metadata(\n",
    "    model_name: str = 'gpt-4o-mini',\n",
    "    temperature: float = 0.0,\n",
    ") -> callable:\n",
    "    \"\"\"Create a custom LLM judge function that returns rich metadata.\"\"\"\n",
    "\n",
    "    async def custom_judge(prompt: str, response_format: type[BaseModel]) -> tuple[BaseModel, dict[str, Any]]:  # noqa: E501\n",
    "        \"\"\"Custom judge with comprehensive metadata tracking.\"\"\"\n",
    "        import time  # noqa: PLC0415\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Create client with specific configuration\n",
    "        client = create_client(\n",
    "            model_name=model_name,\n",
    "            response_format=response_format,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            system_message(\"You are an expert evaluator. Provide thorough assessments.\"),\n",
    "            user_message(prompt),\n",
    "        ]\n",
    "\n",
    "        # Get evaluation from LLM\n",
    "        response = await client.run_async(messages=messages)\n",
    "\n",
    "        # Calculate detailed metadata\n",
    "        end_time = time.time()\n",
    "        metadata = {\n",
    "            # Cost information\n",
    "            \"cost_usd\": response.input_cost + response.output_cost,\n",
    "            \"input_cost_usd\": response.input_cost,\n",
    "            \"output_cost_usd\": response.output_cost,\n",
    "\n",
    "            # Token usage\n",
    "            \"tokens_used\": response.input_tokens + response.output_tokens,\n",
    "            \"input_tokens\": response.input_tokens,\n",
    "            \"output_tokens\": response.output_tokens,\n",
    "\n",
    "            # Performance metrics\n",
    "            \"response_time_ms\": int((end_time - start_time) * 1000),\n",
    "            \"llm_duration_ms\": int(response.duration_seconds * 1000),\n",
    "\n",
    "            # Model information\n",
    "            \"model_version\": model_name,\n",
    "            \"temperature\": temperature,\n",
    "\n",
    "            # Additional context\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S UTC\", time.gmtime()),\n",
    "            \"prompt_length\": len(prompt),\n",
    "        }\n",
    "\n",
    "        return response.parsed, metadata\n",
    "\n",
    "    return custom_judge\n",
    "\n",
    "# Create a custom judge instance\n",
    "custom_judge = await create_custom_judge_with_metadata(temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ð Running evaluation with detailed metadata tracking...\n",
      "\n",
      "â **Judge Response:**\n",
      "   Answers Question: True\n",
      "   Reasoning: The explanation effectively simplifies the complex concept of quantum computing by using a relatable analogy of a maze. It presents the idea of exploring multiple possibilities concurrently, which is a fundamental aspect of quantum computing, making it accessible for beginners.\n",
      "\n",
      "ð **Detailed Metadata:**\n",
      "   cost_usd: $0.000057\n",
      "   tokens_used: 220\n",
      "   input_tokens: 166\n",
      "   output_tokens: 54\n",
      "   response_time_ms: 1250ms\n",
      "   model_version: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Example usage with the custom judge (fixed version)\n",
    "metadata_demo_case = TestCase(\n",
    "    id=\"metadata_demo\",\n",
    "    input=\"Explain quantum computing in simple terms\",\n",
    "    checks=[\n",
    "        LLMJudgeCheck(\n",
    "            prompt=\"Evaluate if this explanation is appropriate for beginners: {{$.output.value}}\",\n",
    "            response_format=BinaryEvaluation,\n",
    "            llm_function=llm_judge_function,  # Use the working judge function instead\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "metadata_demo_output = Output(\n",
    "    value=\"Quantum computing is like having a computer that can explore many possibilities at once, \"  # noqa: E501\n",
    "          \"similar to how you might check multiple paths in a maze simultaneously instead of one at a time.\",  # noqa: E501\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "print(\"ð Running evaluation with detailed metadata tracking...\\n\")\n",
    "demo_result = evaluate([metadata_demo_case], [metadata_demo_output])\n",
    "\n",
    "# Extract and display results\n",
    "check_result = demo_result.results[0].check_results[0]\n",
    "results = check_result.results\n",
    "metadata = results.get('judge_metadata', {})\n",
    "\n",
    "print(\"â **Judge Response:**\")\n",
    "print(f\"   Answers Question: {results.get('answers_question')}\")\n",
    "print(f\"   Reasoning: {results.get('reasoning')}\")\n",
    "\n",
    "print(\"\\nð **Detailed Metadata:**\")\n",
    "for key, value in metadata.items():\n",
    "    if 'cost' in key:\n",
    "        print(f\"   {key}: ${value:.6f}\")\n",
    "    elif 'time' in key or 'duration' in key:\n",
    "        print(f\"   {key}: {value}ms\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "ð¯ **LLM-as-a-Judge Benefits:**\n",
    "- Evaluates subjective qualities that traditional metrics can't capture\n",
    "- Scales to evaluate thousands of responses consistently\n",
    "- Provides detailed, actionable feedback\n",
    "- Adapts to different domains and evaluation criteria\n",
    "\n",
    "ð ï¸ **Implementation Tips:**\n",
    "- Use structured response formats (Pydantic models) for consistency\n",
    "- Provide clear evaluation criteria in your prompts\n",
    "- Include relevant context and examples in templates\n",
    "- Test your evaluation prompts with diverse response types\n",
    "\n",
    "ð **Best Practices:**\n",
    "- Start with simple binary evaluations, then add complexity\n",
    "- Use multiple criteria for comprehensive assessment\n",
    "- Include both strengths and improvement areas in feedback\n",
    "- Validate judge consistency with human evaluations\n",
    "\n",
    "ð **Next Steps:**\n",
    "- Integrate with your AI system's evaluation pipeline\n",
    "- Experiment with different LLM models as judges\n",
    "- Create domain-specific evaluation criteria\n",
    "- Set up automated quality monitoring dashboards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
