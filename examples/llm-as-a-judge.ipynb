{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge Evaluation with Flex-Evals\n",
    "\n",
    "This notebook demonstrates how to use Large Language Models (LLMs) to evaluate AI system outputs using the Flexible Evaluation Protocol (FEP). LLM-as-a-judge is particularly powerful for evaluating subjective qualities like helpfulness, accuracy, tone, and coherence that are difficult to capture with traditional programmatic checks.\n",
    "\n",
    "## Why LLM-as-a-Judge?\n",
    "\n",
    "Traditional evaluation metrics work well for objective criteria (exact matches, thresholds, etc.), but many real-world AI applications need evaluation of subjective qualities:\n",
    "\n",
    "- **Helpfulness**: Is the response useful to the user?\n",
    "- **Accuracy**: Does the response contain correct information?\n",
    "- **Clarity**: Is the explanation easy to understand?\n",
    "- **Completeness**: Does the response fully address the question?\n",
    "- **Tone**: Is the response appropriate for the context?\n",
    "\n",
    "LLM-as-a-judge allows us to evaluate these nuanced criteria at scale while maintaining consistency and auditability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed: pip install flex-evals sik-llms pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from sik_llms import create_client, system_message, user_message\n",
    "\n",
    "from flex_evals import evaluate\n",
    "from flex_evals.schemas import TestCase, Output, Check\n",
    "from flex_evals.constants import CheckType\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # for running async function in a Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Evaluation Criteria\n",
    "\n",
    "We use Pydantic models to define the structure of our evaluation results. This ensures type safety and provides clear documentation of what the LLM judge should evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryEvaluation(BaseModel):\n",
    "    \"\"\"Simple yes/no evaluation with reasoning.\"\"\"\n",
    "\n",
    "    answers_question: bool = Field(description=\"Whether the response answers the user's question\")\n",
    "    reasoning: str = Field(description=\"Brief explanation of the evaluation\")\n",
    "\n",
    "\n",
    "class DetailedQualityAssessment(BaseModel):\n",
    "    \"\"\"Comprehensive quality evaluation with multiple criteria.\"\"\"\n",
    "\n",
    "    overall_score: int = Field(ge=1, le=5, description=\"Overall quality score from 1 (poor) to 5 (excellent)\")  # noqa: E501\n",
    "    helpfulness: int = Field(ge=1, le=5, description=\"How helpful is the response to the user?\")\n",
    "    accuracy: int = Field(ge=1, le=5, description=\"How accurate is the information provided?\")\n",
    "    clarity: int = Field(ge=1, le=5, description=\"How clear and easy to understand is the response?\")  # noqa: E501\n",
    "    completeness: int = Field(ge=1, le=5, description=\"How completely does the response address the question?\")  # noqa: E501\n",
    "    strengths: list[str] = Field(description=\"Key strengths of the response\")\n",
    "    weaknesses: list[str] = Field(description=\"Areas for improvement\")\n",
    "    recommendation: str = Field(description=\"Overall recommendation for this response quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create LLM Judge Function\n",
    "\n",
    "We create a function that uses the sik_llms framework to call an LLM for evaluation. This function will be passed to our LLM judge check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_llm_judge_function(model_name: str = 'gpt-4o-mini') -> callable:\n",
    "    \"\"\"Create an LLM judge function using sik_llms framework.\"\"\"\n",
    "\n",
    "    async def llm_judge(prompt: str, response_format: type[BaseModel]) -> BaseModel:\n",
    "        \"\"\"LLM judge function that evaluates based on the given prompt.\"\"\"\n",
    "        # Create client with the specified response format\n",
    "        client = create_client(\n",
    "            model_name=model_name,\n",
    "            response_format=response_format,\n",
    "        )\n",
    "\n",
    "        # Create messages for the evaluation\n",
    "        messages = [\n",
    "            system_message(\n",
    "                \"You are an expert evaluator tasked with assessing AI responses. \"\n",
    "                \"Provide objective, fair, and constructive evaluations based on the given criteria. \"  # noqa: E501\n",
    "                \"Be specific in your reasoning and provide actionable feedback.\",\n",
    "            ),\n",
    "            user_message(prompt),\n",
    "        ]\n",
    "\n",
    "        # Get evaluation from LLM\n",
    "        response = await client.run_async(messages=messages)\n",
    "        return response.parsed\n",
    "\n",
    "    return llm_judge\n",
    "\n",
    "\n",
    "# Create our judge function\n",
    "llm_judge_function = await create_llm_judge_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Simple Example - Does the AI Answer the Question?\n",
    "\n",
    "Let's start with a simple binary evaluation: does the AI response actually answer the user's question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test cases for question-answering evaluation\n",
    "simple_test_cases = [\n",
    "    TestCase(\n",
    "        id=\"qa_001\",\n",
    "        input=\"What is the capital of France?\",\n",
    "        expected=\"Paris\",  # We know the correct answer\n",
    "        metadata={\"category\": \"geography\", \"difficulty\": \"easy\"},\n",
    "        checks=[\n",
    "            Check(\n",
    "                type=CheckType.LLM_JUDGE,\n",
    "                # Note that we use JSONPath to access test case input and expected values\n",
    "                arguments={\n",
    "                    \"prompt\":\n",
    "                        \"\"\"\n",
    "                        Evaluate whether the AI response properly answers the user's question.\n",
    "\n",
    "                        **User Question:** {{$.test_case.input}}\n",
    "                        **AI Response:** {{$.output.value}}\n",
    "                        **Expected Answer:** {{$.test_case.expected}}\n",
    "\n",
    "                        Consider:\n",
    "                        - Does the response directly address the question?\n",
    "                        - Is the core information correct?\n",
    "                        - Is the response complete enough to be useful?\n",
    "                        \"\"\",\n",
    "                    \"response_format\": BinaryEvaluation,\n",
    "                    \"llm_function\": llm_judge_function,\n",
    "                },\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"qa_002\",\n",
    "        input=\"How do I bake a chocolate cake?\",\n",
    "        expected=\"Step-by-step baking instructions\",\n",
    "        metadata={\"category\": \"cooking\", \"difficulty\": \"medium\"},\n",
    "        checks=[\n",
    "            Check(\n",
    "                type=CheckType.LLM_JUDGE,\n",
    "                arguments={\n",
    "                    \"prompt\":\n",
    "                        \"\"\"\n",
    "                        Evaluate whether the AI response properly answers the user's question.\n",
    "\n",
    "                        **User Question:** {{$.test_case.input}}\n",
    "                        **AI Response:** {{$.output.value}}\n",
    "                        **Expected Type:** {{$.test_case.expected}}\n",
    "\n",
    "                        Consider:\n",
    "                        - Does the response provide actionable instructions?\n",
    "                        - Are the steps clear and in logical order?\n",
    "                        - Would someone be able to follow these instructions?\n",
    "                        \"\"\",\n",
    "                    \"response_format\": BinaryEvaluation,\n",
    "                    \"llm_function\": llm_judge_function,\n",
    "                },\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"qa_003\",\n",
    "        input=\"What's the weather like?\",\n",
    "        expected=\"Request for location or explanation of inability to provide weather\",\n",
    "        metadata={\"category\": \"general\", \"difficulty\": \"easy\"},\n",
    "        checks=[\n",
    "            Check(\n",
    "                type=CheckType.LLM_JUDGE,\n",
    "                arguments={\n",
    "                    \"prompt\":\n",
    "                        \"\"\"\n",
    "                        Evaluate whether the AI response properly handles this question.\n",
    "\n",
    "                        **User Question:** {{$.test_case.input}}\n",
    "                        **AI Response:** {{$.output.value}}\n",
    "                        **Expected Behavior:** {{$.test_case.expected}}\n",
    "\n",
    "                        Consider:\n",
    "                        - Does the AI explain that it needs location information?\n",
    "                        - Or does it explain it cannot access real-time weather data?\n",
    "                        - Is the response helpful despite the limitation?\n",
    "                        \"\"\",\n",
    "                    \"response_format\": BinaryEvaluation,\n",
    "                    \"llm_function\": llm_judge_function,\n",
    "                },\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Simulated AI responses (in practice, these would come from your AI system)\n",
    "simple_outputs = [\n",
    "    Output(\n",
    "        value=\"The capital of France is Paris.\",\n",
    "        metadata={\"model\": \"gpt-4\", \"confidence\": 0.99},\n",
    "    ),\n",
    "    Output(\n",
    "        value=\"To bake a chocolate cake: 1) Preheat oven to 350°F, 2) Mix dry ingredients, 3) Add wet ingredients, 4) Bake for 30-35 minutes. You'll need flour, sugar, cocoa powder, eggs, butter, and baking powder.\",  # noqa: E501\n",
    "        metadata={\"model\": \"gpt-4\", \"confidence\": 0.95},\n",
    "    ),\n",
    "    Output(\n",
    "        value=\"I can't provide current weather information as I don't have access to real-time data. Could you please specify your location and I can suggest ways to check the current weather?\",  # noqa: E501\n",
    "        metadata={\"model\": \"gpt-4\", \"confidence\": 0.90},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running Simple Question-Answering Evaluation...\n",
      "\n",
      "📊 Evaluation Results:\n",
      "   Total test cases: 3\n",
      "   Completed: 3\n",
      "   Errors: 0\n",
      "   Overall status: completed\n",
      "\n",
      "🧪 Test Case: qa_001 (geography)\n",
      "   Question: What is the capital of France?\n",
      "   AI Response: The capital of France is Paris.\n",
      "   ✅ Answers Question: True\n",
      "   💭 Reasoning: The AI response directly answers the user's question by stating that the capital of France is Paris. It provides correct and complete information, making it useful.\n",
      "\n",
      "🧪 Test Case: qa_002 (cooking)\n",
      "   Question: How do I bake a chocolate cake?\n",
      "   AI Response: To bake a chocolate cake: 1) Preheat oven to 350°F, 2) Mix dry ingredients, 3) Add wet ingredients, ...\n",
      "   ✅ Answers Question: True\n",
      "   💭 Reasoning: The response provides actionable, step-by-step instructions for baking a chocolate cake, covering key steps in a clear and logical order. It mentions necessary ingredients and gives specific instructions that someone could follow to successfully bake the cake.\n",
      "\n",
      "🧪 Test Case: qa_003 (general)\n",
      "   Question: What's the weather like?\n",
      "   AI Response: I can't provide current weather information as I don't have access to real-time data. Could you plea...\n",
      "   ✅ Answers Question: True\n",
      "   💭 Reasoning: The AI response effectively addresses the user's question by explaining its limitation in accessing real-time weather data and asking for the user's location to provide alternative suggestions for checking the weather. This fulfills the expected behavior by both clarifying its inability to provide direct information and prompting further engagement.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the simple evaluation\n",
    "print(\"🔍 Running Simple Question-Answering Evaluation...\\n\")\n",
    "\n",
    "simple_results = evaluate(simple_test_cases, simple_outputs)\n",
    "\n",
    "# Display results\n",
    "print(\"📊 Evaluation Results:\")\n",
    "print(f\"   Total test cases: {simple_results.summary.total_test_cases}\")\n",
    "print(f\"   Completed: {simple_results.summary.completed_test_cases}\")\n",
    "print(f\"   Errors: {simple_results.summary.error_test_cases}\")\n",
    "print(f\"   Overall status: {simple_results.status}\\n\")\n",
    "\n",
    "# Show detailed results for each test case\n",
    "for i, result in enumerate(simple_results.results):\n",
    "    test_case = simple_test_cases[i]\n",
    "    check_result = result.check_results[0]\n",
    "    evaluation = check_result.results\n",
    "\n",
    "    print(f\"🧪 Test Case: {test_case.id} ({test_case.metadata['category']})\")\n",
    "    print(f\"   Question: {test_case.input}\")\n",
    "    print(f\"   AI Response: {simple_outputs[i].value[:100]}{'...' if len(simple_outputs[i].value) > 100 else ''}\")  # noqa: E501\n",
    "    print(f\"   ✅ Answers Question: {evaluation['answers_question']}\")\n",
    "    print(f\"   💭 Reasoning: {evaluation['reasoning']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Complex Example - Multi-Criteria Quality Assessment\n",
    "\n",
    "Now let's do a more sophisticated evaluation that assesses multiple quality dimensions and provides detailed feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test cases for detailed quality assessment\n",
    "complex_test_cases = [\n",
    "    TestCase(\n",
    "        id=\"quality_001\",\n",
    "        input={\n",
    "            \"question\": \"Can you explain how machine learning works?\",\n",
    "            \"user_context\": \"I'm a beginner with no technical background\",\n",
    "            \"desired_length\": \"medium explanation\",\n",
    "        },\n",
    "        expected={\n",
    "            \"key_concepts\": [\"training\", \"data\", \"patterns\", \"predictions\"],\n",
    "            \"complexity_level\": \"beginner-friendly\",\n",
    "            \"includes_examples\": True,\n",
    "        },\n",
    "        metadata={\"domain\": \"AI/ML\", \"audience\": \"beginner\"},\n",
    "        checks=[\n",
    "            Check(\n",
    "                type=CheckType.LLM_JUDGE,\n",
    "                arguments={\n",
    "                    \"prompt\": \"\"\"\n",
    "                    Provide a comprehensive quality assessment of this AI response.\n",
    "\n",
    "                    **Context:**\n",
    "                    - User Question: {{$.test_case.input.question}}\n",
    "                    - User Background: {{$.test_case.input.user_context}}\n",
    "                    - Desired Length: {{$.test_case.input.desired_length}}\n",
    "                    - Target Audience: {{$.test_case.metadata.audience}}\n",
    "\n",
    "                    **AI Response to Evaluate:**\n",
    "                    {{$.output.value}}\n",
    "\n",
    "                    **Expected Elements:**\n",
    "                    - Key concepts to cover: {{$.test_case.expected.key_concepts}}\n",
    "                    - Complexity level: {{$.test_case.expected.complexity_level}}\n",
    "                    - Should include examples: {{$.test_case.expected.includes_examples}}\n",
    "\n",
    "                    **Evaluation Criteria:**\n",
    "\n",
    "                    1. **Helpfulness (1-5)**: How useful is this response to someone with the user's background?\n",
    "                    2. **Accuracy (1-5)**: Is the technical information correct and up-to-date?\n",
    "                    3. **Clarity (1-5)**: Is the explanation clear and easy to understand for the target audience?\n",
    "                    4. **Completeness (1-5)**: Does it adequately cover the topic without being overwhelming?\n",
    "\n",
    "                    Provide specific feedback on strengths and areas for improvement.\n",
    "                    \"\"\",  # noqa: E501\n",
    "                    \"response_format\": DetailedQualityAssessment,\n",
    "                    \"llm_function\": llm_judge_function,\n",
    "                },\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"quality_002\",\n",
    "        input={\n",
    "            \"question\": \"How do I fix my code that keeps crashing?\",\n",
    "            \"user_context\": \"Intermediate programmer, Python development\",\n",
    "            \"desired_length\": \"concise but complete\",\n",
    "        },\n",
    "        expected={\n",
    "            \"key_concepts\": [\"debugging\", \"error messages\", \"troubleshooting steps\"],\n",
    "            \"complexity_level\": \"intermediate\",\n",
    "            \"includes_examples\": True,\n",
    "        },\n",
    "        metadata={\"domain\": \"programming\", \"audience\": \"intermediate\"},\n",
    "        checks=[\n",
    "            Check(\n",
    "                type=CheckType.LLM_JUDGE,\n",
    "                arguments={\n",
    "                    \"prompt\": \"\"\"\n",
    "                    Evaluate this programming help response for technical accuracy and usefulness.\n",
    "\n",
    "                    **Context:**\n",
    "                    - User Question: {{$.test_case.input.question}}\n",
    "                    - User Background: {{$.test_case.input.user_context}}\n",
    "                    - Desired Style: {{$.test_case.input.desired_length}}\n",
    "                    - Domain: {{$.test_case.metadata.domain}}\n",
    "\n",
    "                    **AI Response to Evaluate:**\n",
    "                    {{$.output.value}}\n",
    "\n",
    "                    **Expected Elements:**\n",
    "                    - Should cover: {{$.test_case.expected.key_concepts}}\n",
    "                    - Complexity: {{$.test_case.expected.complexity_level}}\n",
    "                    - Examples needed: {{$.test_case.expected.includes_examples}}\n",
    "\n",
    "                    Focus on whether the response provides actionable debugging advice appropriate for an intermediate programmer.\n",
    "                    \"\"\",  # noqa: E501\n",
    "                    \"response_format\": DetailedQualityAssessment,\n",
    "                    \"llm_function\": llm_judge_function,\n",
    "                },\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Simulated AI responses for complex evaluation\n",
    "complex_outputs = [\n",
    "    Output(\n",
    "        value=\"\"\"Machine learning is like teaching a computer to recognize patterns, similar to how you might learn to recognize your friends' faces.\n",
    "\n",
    "Here's how it works in simple terms:\n",
    "\n",
    "1. **Training with Data**: First, we show the computer lots of examples. For instance, if we want it to recognize cats, we show it thousands of cat photos labeled \"cat.\"\n",
    "\n",
    "2. **Finding Patterns**: The computer looks for common features in all those cat photos - things like pointy ears, whiskers, or fur patterns.\n",
    "\n",
    "3. **Making Predictions**: When you show it a new photo, it uses those patterns to guess whether it's a cat or not.\n",
    "\n",
    "Think of it like learning to cook - the more recipes you follow (training data), the better you get at creating new dishes (making predictions). The computer gets \"smarter\" by seeing more examples, just like you get better at cooking with practice!\n",
    "\n",
    "This same process helps computers do amazing things like recommend movies on Netflix, translate languages, or even help doctors diagnose diseases.\"\"\",  # noqa: E501\n",
    "        metadata={\"model\": \"gpt-4\", \"response_time_ms\": 1200, \"confidence\": 0.92},\n",
    "    ),\n",
    "    Output(\n",
    "        value=\"\"\"Here's a systematic approach to debug crashing Python code:\n",
    "\n",
    "1. **Read the error message carefully** - Python error messages usually tell you exactly what's wrong and where.\n",
    "\n",
    "2. **Check the stack trace** - Look at the last few lines to see which line of code caused the crash.\n",
    "\n",
    "3. **Common crash causes**:\n",
    "   - `NameError`: Variable not defined\n",
    "   - `IndexError`: Accessing list element that doesn't exist\n",
    "   - `KeyError`: Dictionary key not found\n",
    "   - `TypeError`: Wrong data type for operation\n",
    "\n",
    "4. **Debugging techniques**:\n",
    "   ```python\n",
    "   # Add print statements to trace execution\n",
    "   print(f\"Variable x = {x}\")\n",
    "\n",
    "   # Use try-except to catch specific errors\n",
    "   try:\n",
    "       risky_operation()\n",
    "   except IndexError as e:\n",
    "       print(f\"Index error: {e}\")\n",
    "   ```\n",
    "\n",
    "5. **Use a debugger** - Try `pdb.set_trace()` or your IDE's debugger to step through code line by line.\n",
    "\n",
    "If you share the specific error message, I can give more targeted advice!\"\"\",  # noqa: E501\n",
    "        metadata={\"model\": \"gpt-4\", \"response_time_ms\": 800, \"confidence\": 0.88},\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running Detailed Quality Assessment...\n",
      "\n",
      "📊 Evaluation Results:\n",
      "   Total test cases: 2\n",
      "   Completed: 2\n",
      "   Errors: 0\n",
      "   Overall status: completed\n",
      "\n",
      "🧪 Test Case: quality_001 (AI/ML)\n",
      "   Question: Can you explain how machine learning works?\n",
      "   User Context: I'm a beginner with no technical background\n",
      "\n",
      "   📊 **Quality Scores:**\n",
      "      Overall: 4/5\n",
      "      Helpfulness: 4/5\n",
      "      Accuracy: 5/5\n",
      "      Clarity: 4/5\n",
      "      Completeness: 4/5\n",
      "\n",
      "   ✅ **Strengths:**\n",
      "      • Uses relatable analogies (friends' faces, cooking) that make complex concepts accessible to beginners.\n",
      "      • Clearly outlines the three major steps in machine learning: training with data, finding patterns, and making predictions.\n",
      "      • Includes examples of real-world applications of machine learning, enhancing the relevance of the explanation.\n",
      "\n",
      "   ⚠️  **Areas for Improvement:**\n",
      "      • Could include a brief mention of the importance of data quality and variety in the training phase to provide a more complete understanding.\n",
      "      • The explanation might benefit from a more structured format or bullet points to enhance readability, particularly for beginners.\n",
      "\n",
      "   💡 **Recommendation:** Consider revising the response to include more emphasis on data quality and structure for enhanced readability.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "🧪 Test Case: quality_002 (programming)\n",
      "   Question: How do I fix my code that keeps crashing?\n",
      "   User Context: Intermediate programmer, Python development\n",
      "\n",
      "   📊 **Quality Scores:**\n",
      "      Overall: 5/5\n",
      "      Helpfulness: 5/5\n",
      "      Accuracy: 5/5\n",
      "      Clarity: 5/5\n",
      "      Completeness: 5/5\n",
      "\n",
      "   ✅ **Strengths:**\n",
      "      • Systematic approach to debugging is clearly outlined.\n",
      "      • Identifies common Python error types relevant to crashing code which is very useful for the user's level.\n",
      "      • Includes practical examples of debugging techniques like print statements and try-except blocks.\n",
      "      • Encourages user interaction by suggesting they provide specific error messages for targeted advice.\n",
      "\n",
      "   ⚠️  **Areas for Improvement:**\n",
      "\n",
      "   💡 **Recommendation:** This response is well-structured and effectively addresses the user's needs. It should maintain this level of comprehensiveness and clarity in future responses.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the complex evaluation\n",
    "print(\"🔍 Running Detailed Quality Assessment...\\n\")\n",
    "\n",
    "complex_results = evaluate(complex_test_cases, complex_outputs)\n",
    "\n",
    "# Display results\n",
    "print(\"📊 Evaluation Results:\")\n",
    "print(f\"   Total test cases: {complex_results.summary.total_test_cases}\")\n",
    "print(f\"   Completed: {complex_results.summary.completed_test_cases}\")\n",
    "print(f\"   Errors: {complex_results.summary.error_test_cases}\")\n",
    "print(f\"   Overall status: {complex_results.status}\\n\")\n",
    "\n",
    "# Show detailed results for each test case\n",
    "for i, result in enumerate(complex_results.results):\n",
    "    test_case = complex_test_cases[i]\n",
    "    check_result = result.check_results[0]\n",
    "    evaluation = check_result.results\n",
    "\n",
    "    print(f\"🧪 Test Case: {test_case.id} ({test_case.metadata['domain']})\")\n",
    "    print(f\"   Question: {test_case.input['question']}\")\n",
    "    print(f\"   User Context: {test_case.input['user_context']}\")\n",
    "    print()\n",
    "    print(\"   📊 **Quality Scores:**\")\n",
    "    print(f\"      Overall: {evaluation['overall_score']}/5\")\n",
    "    print(f\"      Helpfulness: {evaluation['helpfulness']}/5\")\n",
    "    print(f\"      Accuracy: {evaluation['accuracy']}/5\")\n",
    "    print(f\"      Clarity: {evaluation['clarity']}/5\")\n",
    "    print(f\"      Completeness: {evaluation['completeness']}/5\")\n",
    "    print()\n",
    "    print(\"   ✅ **Strengths:**\")\n",
    "    for strength in evaluation['strengths']:\n",
    "        print(f\"      • {strength}\")\n",
    "    print()\n",
    "    print(\"   ⚠️  **Areas for Improvement:**\")\n",
    "    for weakness in evaluation['weaknesses']:\n",
    "        print(f\"      • {weakness}\")\n",
    "    print()\n",
    "    print(f\"   💡 **Recommendation:** {evaluation['recommendation']}\")\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Batch Evaluation Example\n",
    "\n",
    "Let's demonstrate how to evaluate multiple AI responses efficiently, which is useful for:\n",
    "- A/B testing different models\n",
    "- Evaluating prompt engineering changes\n",
    "- Quality assurance on production responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of customer service responses to evaluate\n",
    "customer_service_cases = [\n",
    "    TestCase(\n",
    "        id=\"cs_001\",\n",
    "        input=\"I'm really frustrated! My order hasn't arrived and it's been a week. What's going on?\",  # noqa: E501\n",
    "        metadata={\"sentiment\": \"angry\", \"issue_type\": \"delivery_delay\"},\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"cs_002\",\n",
    "        input=\"Hi, I need to return this item. It doesn't fit properly. What's your return policy?\",  # noqa: E501\n",
    "        metadata={\"sentiment\": \"neutral\", \"issue_type\": \"return_request\"},\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"cs_003\",\n",
    "        input=\"Your product is amazing! Just wanted to say thanks. Also, do you have any similar products?\",  # noqa: E501\n",
    "        metadata={\"sentiment\": \"positive\", \"issue_type\": \"product_inquiry\"},\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"cs_004\",\n",
    "        input=\"I was charged twice for the same order. This is unacceptable. I want a refund immediately.\",  # noqa: E501\n",
    "        metadata={\"sentiment\": \"very_angry\", \"issue_type\": \"billing_error\"},\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"cs_005\",\n",
    "        input=\"Quick question - what are your business hours? I want to call later.\",\n",
    "        metadata={\"sentiment\": \"neutral\", \"issue_type\": \"information_request\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Add the same evaluation check to all test cases\n",
    "customer_service_check = Check(\n",
    "    type=CheckType.LLM_JUDGE,\n",
    "    arguments={\n",
    "        \"prompt\": \"\"\"\n",
    "        Evaluate this customer service response for quality and appropriateness.\n",
    "\n",
    "        **Customer Message:** {{$.test_case.input}}\n",
    "        **Customer Sentiment:** {{$.test_case.metadata.sentiment}}\n",
    "        **Issue Type:** {{$.test_case.metadata.issue_type}}\n",
    "\n",
    "        **AI Response:** {{$.output.value}}\n",
    "\n",
    "        **Evaluation Criteria:**\n",
    "        - **Helpfulness**: Does the response address the customer's needs?\n",
    "        - **Empathy**: Is the tone appropriate for the customer's emotional state?\n",
    "        - **Professionalism**: Is the response professional and courteous?\n",
    "        - **Actionability**: Does it provide clear next steps or solutions?\n",
    "\n",
    "        Rate each dimension and provide an overall assessment.\n",
    "        \"\"\",\n",
    "        \"response_format\": DetailedQualityAssessment,\n",
    "        \"llm_function\": llm_judge_function,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Apply the same check to all test cases\n",
    "for test_case in customer_service_cases:\n",
    "    test_case.checks = [customer_service_check]\n",
    "\n",
    "# Simulated customer service AI responses\n",
    "customer_service_outputs = [\n",
    "    Output(value=\"I sincerely apologize for the delay with your order. I understand how frustrating this must be. Let me look into this immediately and provide you with a tracking update and expedited shipping at no cost.\"),  # noqa: E501\n",
    "    Output(value=\"Of course! Our return policy allows returns within 30 days. Since the fit isn't right, you can return it for a full refund or exchange. I'll email you a prepaid return label right now.\"),  # noqa: E501\n",
    "    Output(value=\"Thank you so much for the kind words! We're thrilled you love the product. Based on what you purchased, I think you'd really like our Premium Series - I'll send you some recommendations!\"),  # noqa: E501\n",
    "    Output(value=\"I apologize for this billing error - that's definitely not acceptable. I've immediately processed a refund for the duplicate charge, and you should see it in 2-3 business days. I've also added a credit to your account.\"),  # noqa: E501\n",
    "    Output(value=\"Our customer service hours are Monday-Friday 8 AM to 8 PM EST, and Saturday-Sunday 10 AM to 6 PM EST. You can also reach us anytime through our 24/7 chat support!\"),  # noqa: E501\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running Customer Service Quality Batch Evaluation...\n",
      "\n",
      "📊 **Customer Service Response Quality Report**\n",
      "\n",
      "| Test ID | Sentiment | Issue Type | Overall | Help | Accuracy | Clarity | Complete |\n",
      "|---------|-----------|------------|---------|------|----------|---------|----------|\n",
      "| cs_001 | angry | delivery_d | 4/5 | 4/5 | 5/5 | 5/5 | 4/5 |\n",
      "| cs_002 | neutral | return_req | 5/5 | 5/5 | 5/5 | 5/5 | 5/5 |\n",
      "| cs_003 | positive | product_in | 4/5 | 5/5 | 4/5 | 5/5 | 4/5 |\n",
      "| cs_004 | very_ang | billing_er | 4/5 | 5/5 | 5/5 | 5/5 | 4/5 |\n",
      "| cs_005 | neutral | informatio | 5/5 | 5/5 | 5/5 | 5/5 | 5/5 |\n",
      "\n",
      "📈 **Average Scores Across All Responses:**\n",
      "   Overall: 4.40/5\n",
      "   Helpfulness: 4.80/5\n",
      "   Accuracy: 4.80/5\n",
      "   Clarity: 5.00/5\n",
      "   Completeness: 4.40/5\n",
      "\n",
      "🏆 **Best Response:** cs_002 (Score: 5/5)\n",
      "⚠️  **Needs Improvement:** cs_001 (Score: 4/5)\n"
     ]
    }
   ],
   "source": [
    "# Run batch evaluation\n",
    "print(\"🔍 Running Customer Service Quality Batch Evaluation...\\n\")\n",
    "\n",
    "batch_results = evaluate(customer_service_cases, customer_service_outputs)\n",
    "\n",
    "# Calculate average scores across all responses\n",
    "all_scores = {\n",
    "    'overall': [],\n",
    "    'helpfulness': [],\n",
    "    'accuracy': [],\n",
    "    'clarity': [],\n",
    "    'completeness': [],\n",
    "}\n",
    "\n",
    "print(\"📊 **Customer Service Response Quality Report**\\n\")\n",
    "print(\"| Test ID | Sentiment | Issue Type | Overall | Help | Accuracy | Clarity | Complete |\")\n",
    "print(\"|---------|-----------|------------|---------|------|----------|---------|----------|\")\n",
    "\n",
    "for i, result in enumerate(batch_results.results):\n",
    "    test_case = customer_service_cases[i]\n",
    "    evaluation = result.check_results[0].results\n",
    "\n",
    "    # Collect scores for averaging\n",
    "    all_scores['overall'].append(evaluation['overall_score'])\n",
    "    all_scores['helpfulness'].append(evaluation['helpfulness'])\n",
    "    all_scores['accuracy'].append(evaluation['accuracy'])\n",
    "    all_scores['clarity'].append(evaluation['clarity'])\n",
    "    all_scores['completeness'].append(evaluation['completeness'])\n",
    "\n",
    "    # Display in table format\n",
    "    print(f\"| {test_case.id} | {test_case.metadata['sentiment'][:8]} | {test_case.metadata['issue_type'][:10]} | \"  # noqa: E501\n",
    "          f\"{evaluation['overall_score']}/5 | {evaluation['helpfulness']}/5 | {evaluation['accuracy']}/5 | \"  # noqa: E501\n",
    "          f\"{evaluation['clarity']}/5 | {evaluation['completeness']}/5 |\")\n",
    "\n",
    "# Calculate and display averages\n",
    "print(\"\\n📈 **Average Scores Across All Responses:**\")\n",
    "for metric, scores in all_scores.items():\n",
    "    avg_score = sum(scores) / len(scores)\n",
    "    print(f\"   {metric.title()}: {avg_score:.2f}/5\")\n",
    "\n",
    "# Find best and worst performing responses\n",
    "overall_scores = all_scores['overall']\n",
    "best_idx = overall_scores.index(max(overall_scores))\n",
    "worst_idx = overall_scores.index(min(overall_scores))\n",
    "\n",
    "print(f\"\\n🏆 **Best Response:** {customer_service_cases[best_idx].id} (Score: {overall_scores[best_idx]}/5)\")  # noqa: E501\n",
    "print(f\"⚠️  **Needs Improvement:** {customer_service_cases[worst_idx].id} (Score: {overall_scores[worst_idx]}/5)\")  # noqa: E501\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Advanced Features\n",
    "\n",
    "### Template Processing Power\n",
    "\n",
    "Notice how we used `{{$.jsonpath}}` expressions in our prompts! This powerful feature allows dynamic prompt generation:\n",
    "\n",
    "- `{{$.test_case.input}}` - Access the test input\n",
    "- `{{$.output.value}}` - Access the AI response\n",
    "- `{{$.test_case.metadata.sentiment}}` - Access nested metadata\n",
    "- `{{$.output.metadata.confidence}}` - Access response metadata\n",
    "\n",
    "### Error Handling and Robustness\n",
    "\n",
    "The framework automatically handles:\n",
    "- Invalid JSONPath expressions\n",
    "- LLM API failures\n",
    "- Response format validation errors\n",
    "- Network timeouts and retries\n",
    "\n",
    "### Extensibility\n",
    "\n",
    "You can easily extend this for your use cases:\n",
    "- Custom response formats for domain-specific evaluation\n",
    "- Different LLM models for different types of evaluation\n",
    "- Integration with your existing evaluation pipelines\n",
    "- Automated report generation and alerting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "🎯 **LLM-as-a-Judge Benefits:**\n",
    "- Evaluates subjective qualities that traditional metrics can't capture\n",
    "- Scales to evaluate thousands of responses consistently\n",
    "- Provides detailed, actionable feedback\n",
    "- Adapts to different domains and evaluation criteria\n",
    "\n",
    "🛠️ **Implementation Tips:**\n",
    "- Use structured response formats (Pydantic models) for consistency\n",
    "- Provide clear evaluation criteria in your prompts\n",
    "- Include relevant context and examples in templates\n",
    "- Test your evaluation prompts with diverse response types\n",
    "\n",
    "📊 **Best Practices:**\n",
    "- Start with simple binary evaluations, then add complexity\n",
    "- Use multiple criteria for comprehensive assessment\n",
    "- Include both strengths and improvement areas in feedback\n",
    "- Validate judge consistency with human evaluations\n",
    "\n",
    "🚀 **Next Steps:**\n",
    "- Integrate with your AI system's evaluation pipeline\n",
    "- Experiment with different LLM models as judges\n",
    "- Create domain-specific evaluation criteria\n",
    "- Set up automated quality monitoring dashboards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
