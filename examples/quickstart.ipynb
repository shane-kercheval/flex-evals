{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEP Quickstart Guide\n",
    "\n",
    "This notebook provides a simple introduction to the Flexible Evaluation Protocol (FEP) - a vendor-neutral, schema-driven standard for evaluating any system that produces complex or variable outputs.\n",
    "\n",
    "## What is FEP?\n",
    "\n",
    "FEP provides:\n",
    "- **Portable data formats** for test cases, system outputs, and results\n",
    "- **Pluggable \"checks\"** for boolean, numeric, categorical, and free-text assessments\n",
    "- **JSONPath expressions** for flexible data extraction and comparison\n",
    "\n",
    "Let's see it in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup\n",
    "\n",
    "If you haven't already, install the flex-evals package into your environment: `pip install flex-evals`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the core FEP components\n",
    "from flex_evals import (\n",
    "    evaluate,\n",
    "    TestCase,\n",
    "    Output,\n",
    "    Check,\n",
    "    CheckType,\n",
    ")\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Simple Text Evaluation\n",
    "\n",
    "Let's start with a basic question-answering evaluation where we check if a system gives the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Cases:\n",
      "  geography_001: What is the capital of France? → Expected: Paris\n",
      "  math_001: What is 2 + 2? → Expected: 4\n",
      "\n",
      "System Outputs:\n",
      "  geography_001: Paris\n",
      "  math_001: Four\n"
     ]
    }
   ],
   "source": [
    "# Define our test cases\n",
    "test_cases = [\n",
    "    TestCase(\n",
    "        id=\"geography_001\",\n",
    "        input=\"What is the capital of France?\",\n",
    "        expected=\"Paris\",\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"math_001\",\n",
    "        input=\"What is 2 + 2?\",\n",
    "        expected=\"4\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Simulate system outputs (in reality, these would come from your LLM/API)\n",
    "outputs = [\n",
    "    Output(value=\"Paris\"),\n",
    "    Output(value=\"Four\"),\n",
    "]\n",
    "\n",
    "print(\"Test Cases:\")\n",
    "for tc in test_cases:\n",
    "    print(f\"  {tc.id}: {tc.input} → Expected: {tc.expected}\")\n",
    "\n",
    "print(\"\\nSystem Outputs:\")\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"  {test_cases[i].id}: {output.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Check:\n",
      "  Type: exact_match\n",
      "  Case Sensitive: False\n",
      "  Pattern: Shared checks (same check applied to all 2 test cases)\n"
     ]
    }
   ],
   "source": [
    "# Define checks to evaluate the outputs\n",
    "#\n",
    "# FEP supports two different patterns for organizing checks:\n",
    "#\n",
    "# 1. **Shared Checks (List[Check])** - Same checks applied to ALL test cases\n",
    "#    This is what we're using here - the exact_match check will be applied to both test cases\n",
    "#    This is the \"1-to-many\" pattern from the FEP specification\n",
    "#\n",
    "# 2. **Per-Test-Case Checks (List[List[Check]])** - Each test case has its own checks\n",
    "#    Example: [[check1, check2], [check3, check4]] where first list applies to test_cases[0]\n",
    "#    This allows different evaluation criteria per test case\n",
    "#\n",
    "# Note: This Python implementation also supports a convenience feature where you can\n",
    "# pass checks=None and embed checks directly in TestCase objects using TestCase.checks,\n",
    "# but this is not part of the core FEP specification - it's a Python-specific extension.\n",
    "#\n",
    "# The shared pattern (used below) is perfect when you want to apply the same\n",
    "# evaluation criteria across all test cases, like checking that all responses\n",
    "# match their expected values.\n",
    "\n",
    "checks = [\n",
    "    # this check can be shared across all test cases since it referencesthe test case's expected\n",
    "    # value dynamically using JSONPath\n",
    "    Check(\n",
    "        type=CheckType.EXACT_MATCH,\n",
    "        arguments={\n",
    "            \"actual\": \"$.output.value\",        # JSONPath to extract actual output\n",
    "            \"expected\": \"$.test_case.expected\", # JSONPath to extract expected output\n",
    "            \"case_sensitive\": False,            # Allow case-insensitive matching\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Evaluation Check:\")\n",
    "print(f\"  Type: {checks[0].type}\")\n",
    "print(f\"  Case Sensitive: {checks[0].arguments['case_sensitive']}\")\n",
    "print(f\"  Pattern: Shared checks (same check applied to all {len(test_cases)} test cases)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation completed in 0.011 seconds\n",
      "Status: completed\n",
      "Summary: 2/2 test cases completed\n"
     ]
    }
   ],
   "source": [
    "# Run the evaluation\n",
    "result = evaluate(\n",
    "    test_cases=test_cases,\n",
    "    outputs=outputs,\n",
    "    checks=checks,\n",
    ")\n",
    "\n",
    "print(f\"Evaluation completed in {(result.completed_at - result.started_at).total_seconds():.3f} seconds\")  # noqa: E501\n",
    "print(f\"Status: {result.status}\")\n",
    "print(f\"Summary: {result.summary.completed_test_cases}/{result.summary.total_test_cases} test cases completed\")  # noqa: E501\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Results:\n",
      "==================================================\n",
      "\n",
      "Test Case: geography_001\n",
      "Status: completed\n",
      "\n",
      "  Check: exact_match\n",
      "  Passed: True\n",
      "  Actual: 'Paris'\n",
      "  Expected: 'Paris'\n",
      "\n",
      "Test Case: math_001\n",
      "Status: completed\n",
      "\n",
      "  Check: exact_match\n",
      "  Passed: False\n",
      "  Actual: 'Four'\n",
      "  Expected: '4'\n"
     ]
    }
   ],
   "source": [
    "# Examine the detailed results\n",
    "print(\"Detailed Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for test_result in result.results:\n",
    "    print(f\"\\nTest Case: {test_result.test_case_id}\")\n",
    "    print(f\"Status: {test_result.status}\")\n",
    "    for check_result in test_result.check_results:\n",
    "        print(f\"\\n  Check: {check_result.check_type}\")\n",
    "        print(f\"  Passed: {check_result.results['passed']}\")\n",
    "        print(f\"  Actual: '{check_result.resolved_arguments['actual']['value']}'\")\n",
    "        print(f\"  Expected: '{check_result.resolved_arguments['expected']['value']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Multiple Check Types\n",
    "\n",
    "Let's use different types of checks to evaluate various aspects of system outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case:\n",
      "  ID: coding_001\n",
      "  Input: Write a Python function that returns the square of a number\n",
      "  Metadata: {'difficulty': 'easy', 'language': 'python'}\n",
      "\n",
      "System Output:\n",
      "  Code: def square(x):\n",
      "    return x * ...\n",
      "  Confidence: 0.95\n",
      "  Execution Time: 150ms\n"
     ]
    }
   ],
   "source": [
    "# Test case for a coding question\n",
    "coding_test = TestCase(\n",
    "    id=\"coding_001\",\n",
    "    input=\"Write a Python function that returns the square of a number\",\n",
    "    metadata={\"difficulty\": \"easy\", \"language\": \"python\"},\n",
    ")\n",
    "\n",
    "# Simulated system output with structured data\n",
    "coding_output = Output(\n",
    "    value={\n",
    "        \"code\": \"def square(x):\\n    return x * x\",\n",
    "        \"explanation\": \"This function takes a number x and returns its square by multiplying it by itself.\",  # noqa: E501\n",
    "        \"confidence\": 0.95,\n",
    "    },\n",
    "    metadata={\"execution_time_ms\": 150, \"model\": \"gpt-4\"},\n",
    ")\n",
    "\n",
    "print(\"Test Case:\")\n",
    "print(f\"  ID: {coding_test.id}\")\n",
    "print(f\"  Input: {coding_test.input}\")\n",
    "print(f\"  Metadata: {coding_test.metadata}\")\n",
    "\n",
    "print(\"\\nSystem Output:\")\n",
    "print(f\"  Code: {coding_output.value['code'][:30]}...\")\n",
    "print(f\"  Confidence: {coding_output.value['confidence']}\")\n",
    "print(f\"  Execution Time: {coding_output.metadata['execution_time_ms']}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Checks:\n",
      "  1. contains - ['text', 'phrases']\n",
      "  2. regex - ['text', 'pattern', 'flags']\n",
      "  3. threshold - ['value', 'min_value']\n",
      "  4. threshold - ['value', 'max_value']\n"
     ]
    }
   ],
   "source": [
    "# Define multiple checks for different aspects\n",
    "multi_checks = [\n",
    "    # Check 1: Code contains the word \"def\" (function definition)\n",
    "    Check(\n",
    "        type=CheckType.CONTAINS,\n",
    "        arguments={\n",
    "            \"text\": \"$.output.value.code\",\n",
    "            \"phrases\": [\"def\"],\n",
    "        },\n",
    "    ),\n",
    "\n",
    "    # Check 2: Code matches function pattern with regex\n",
    "    Check(\n",
    "        type=CheckType.REGEX,\n",
    "        arguments={\n",
    "            \"text\": \"$.output.value.code\",\n",
    "            \"pattern\": r\"def\\s+\\w+\\s*\\(\",  # Function definition pattern\n",
    "            \"flags\": {\"case_insensitive\": True},\n",
    "        },\n",
    "    ),\n",
    "\n",
    "    # Check 3: Confidence is above threshold\n",
    "    Check(\n",
    "        type=CheckType.THRESHOLD,\n",
    "        arguments={\n",
    "            \"value\": \"$.output.value.confidence\",\n",
    "            \"min_value\": 0.8,\n",
    "        },\n",
    "    ),\n",
    "\n",
    "    # Check 4: Execution time is reasonable\n",
    "    Check(\n",
    "        type=CheckType.THRESHOLD,\n",
    "        arguments={\n",
    "            \"value\": \"$.output.metadata.execution_time_ms\",\n",
    "            \"max_value\": 1000,\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Evaluation Checks:\")\n",
    "for i, check in enumerate(multi_checks, 1):\n",
    "    print(f\"  {i}. {check.type} - {list(check.arguments.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Check Evaluation Results:\n",
      "========================================\n",
      "\n",
      "Test Case: coding_001\n",
      "Overall Status: completed\n",
      "Checks Passed: 4/4\n",
      "\n",
      "Individual Check Results:\n",
      "  1. ✓ contains: True\n",
      "  2. ✓ regex: True\n",
      "  3. ✓ threshold: True\n",
      "     Value: 0.95\n",
      "  4. ✓ threshold: True\n",
      "     Value: 150\n"
     ]
    }
   ],
   "source": [
    "# Run the multi-check evaluation\n",
    "multi_result = evaluate(\n",
    "    test_cases=[coding_test],\n",
    "    outputs=[coding_output],\n",
    "    checks=multi_checks,\n",
    ")\n",
    "\n",
    "print(\"Multi-Check Evaluation Results:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "test_result = multi_result.results[0]\n",
    "print(f\"\\nTest Case: {test_result.test_case_id}\")\n",
    "print(f\"Overall Status: {test_result.status}\")\n",
    "print(f\"Checks Passed: {test_result.summary.completed_checks}/{test_result.summary.total_checks}\")\n",
    "\n",
    "print(\"\\nIndividual Check Results:\")\n",
    "for i, check_result in enumerate(test_result.check_results, 1):\n",
    "    status_icon = \"✓\" if check_result.results.get('passed', False) else \"✗\"\n",
    "    print(f\"  {i}. {status_icon} {check_result.check_type}: {check_result.results.get('passed', 'N/A')}\")  # noqa: E501\n",
    "\n",
    "    # Show resolved values for context\n",
    "    if 'value' in check_result.resolved_arguments:\n",
    "        value = check_result.resolved_arguments['value']['value']\n",
    "        print(f\"     Value: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Complex Structured Output\n",
    "\n",
    "FEP excels at evaluating complex, nested outputs using JSONPath expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex Agent Output:\n",
      "{\n",
      "  \"plan\": {\n",
      "    \"destination\": \"Paris, France\",\n",
      "    \"duration_days\": 3,\n",
      "    \"budget_usd\": 1500,\n",
      "    \"itinerary\": [\n",
      "      {\n",
      "        \"day\": 1,\n",
      "        \"activities\": [\n",
      "          \"Visit Eiffel Tower\",\n",
      "          \"Louvre Museum\"\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"day\": 2,\n",
      "        \"activities\": [\n",
      "      ...\n"
     ]
    }
   ],
   "source": [
    "# Test case for an AI agent that needs to plan a task\n",
    "agent_test = TestCase(\n",
    "    id=\"agent_001\",\n",
    "    input=\"Plan a trip to Paris for 3 days\",\n",
    "    expected={\n",
    "        \"duration_days\": 3,\n",
    "        \"destination\": \"Paris\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Complex structured output from an AI agent\n",
    "agent_output = Output(\n",
    "    value={\n",
    "        \"plan\": {\n",
    "            \"destination\": \"Paris, France\",\n",
    "            \"duration_days\": 3,\n",
    "            \"budget_usd\": 1500,\n",
    "            \"itinerary\": [\n",
    "                {\"day\": 1, \"activities\": [\"Visit Eiffel Tower\", \"Louvre Museum\"]},\n",
    "                {\"day\": 2, \"activities\": [\"Notre-Dame\", \"Seine River Cruise\"]},\n",
    "                {\"day\": 3, \"activities\": [\"Versailles\", \"Shopping\"]},\n",
    "            ],\n",
    "        },\n",
    "        \"tools_used\": [\"travel_search\", \"hotel_booking\", \"activity_planner\"],\n",
    "        \"confidence\": 0.92,\n",
    "        \"reasoning\": \"Created a balanced 3-day itinerary covering major Paris attractions\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Complex Agent Output:\")\n",
    "print(json.dumps(agent_output.value, indent=2)[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex Evaluation Checks:\n",
      "  1. contains: text = $.output.value.plan.destination\n",
      "  2. exact_match: actual = $.output.value.plan.duration_days\n",
      "  3. threshold: value = $.output.value.plan.budget_usd\n",
      "  4. threshold: value = $.output.value.plan.itinerary.length\n",
      "  5. contains: text = $.output.value.tools_used\n"
     ]
    }
   ],
   "source": [
    "# Define checks using JSONPath for complex data extraction\n",
    "complex_checks = [\n",
    "    # Check destination contains \"Paris\"\n",
    "    Check(\n",
    "        type=CheckType.CONTAINS,\n",
    "        arguments={\n",
    "            \"text\": \"$.output.value.plan.destination\",\n",
    "            \"phrases\": [\"Paris\"],\n",
    "            \"case_sensitive\": False,\n",
    "        },\n",
    "    ),\n",
    "\n",
    "    # Check duration is exactly 3 days\n",
    "    Check(\n",
    "        type=CheckType.EXACT_MATCH,\n",
    "        arguments={\n",
    "            \"actual\": \"$.output.value.plan.duration_days\",\n",
    "            \"expected\": \"$.test_case.expected.duration_days\",\n",
    "        },\n",
    "    ),\n",
    "\n",
    "    # Check that budget is reasonable (between $500-$3000)\n",
    "    Check(\n",
    "        type=CheckType.THRESHOLD,\n",
    "        arguments={\n",
    "            \"value\": \"$.output.value.plan.budget_usd\",\n",
    "            \"min_value\": 500,\n",
    "            \"max_value\": 3000,\n",
    "        },\n",
    "    ),\n",
    "\n",
    "    # Check that itinerary has 3 days\n",
    "    Check(\n",
    "        type=CheckType.THRESHOLD,\n",
    "        arguments={\n",
    "            \"value\": \"$.output.value.plan.itinerary.length\",  # JSONPath length function\n",
    "            \"min_value\": 3,\n",
    "            \"max_value\": 3,\n",
    "        },\n",
    "    ),\n",
    "\n",
    "    # Check that agent used appropriate tools\n",
    "    Check(\n",
    "        type=CheckType.CONTAINS,\n",
    "        arguments={\n",
    "            \"text\": \"$.output.value.tools_used\",\n",
    "            \"phrases\": [\"travel_search\"],\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Complex Evaluation Checks:\")\n",
    "for i, check in enumerate(complex_checks, 1):\n",
    "    key_arg = next(iter(check.arguments.keys()))\n",
    "    print(f\"  {i}. {check.type}: {key_arg} = {check.arguments[key_arg]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex Evaluation Results:\n",
      "========================================\n",
      "Test Case: agent_001\n",
      "Overall Status: error\n",
      "Success Rate: 4/5\n",
      "\n",
      "Detailed Check Results:\n",
      "\n",
      "  1. ✓ contains\n",
      "     text: Paris, France (from $.output.value.plan.destination)\n",
      "     phrases: ['Paris']\n",
      "     case_sensitive: False\n",
      "\n",
      "  2. ✓ exact_match\n",
      "     actual: 3 (from $.output.value.plan.duration_days)\n",
      "     expected: 3 (from $.test_case.expected.duration_days)\n",
      "\n",
      "  3. ✓ threshold\n",
      "     value: 1500 (from $.output.value.plan.budget_usd)\n",
      "     min_value: 500\n",
      "     max_value: 3000\n",
      "\n",
      "  4. ✗ threshold\n",
      "\n",
      "  5. ✓ contains\n",
      "     text: ['travel_search', 'hotel_booking', 'activity_planner'] (from $.output.value.tools_used)\n",
      "     phrases: ['travel_search']\n"
     ]
    }
   ],
   "source": [
    "# Run the complex evaluation\n",
    "complex_result = evaluate(\n",
    "    test_cases=[agent_test],\n",
    "    outputs=[agent_output],\n",
    "    checks=complex_checks,\n",
    ")\n",
    "\n",
    "print(\"Complex Evaluation Results:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "test_result = complex_result.results[0]\n",
    "print(f\"Test Case: {test_result.test_case_id}\")\n",
    "print(f\"Overall Status: {test_result.status}\")\n",
    "print(f\"Success Rate: {test_result.summary.completed_checks}/{test_result.summary.total_checks}\")\n",
    "\n",
    "print(\"\\nDetailed Check Results:\")\n",
    "for i, check_result in enumerate(test_result.check_results, 1):\n",
    "    passed = check_result.results.get('passed', False)\n",
    "    status_icon = \"✓\" if passed else \"✗\"\n",
    "    print(f\"\\n  {i}. {status_icon} {check_result.check_type}\")\n",
    "\n",
    "    # Show what was actually evaluated\n",
    "    for arg_name, arg_data in check_result.resolved_arguments.items():\n",
    "        if 'jsonpath' in arg_data:\n",
    "            print(f\"     {arg_name}: {arg_data['value']} (from {arg_data['jsonpath']})\")\n",
    "        else:\n",
    "            print(f\"     {arg_name}: {arg_data['value']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "From these examples, you can see how FEP provides:\n",
    "\n",
    "1. **Flexible Data Extraction**: JSONPath expressions like `$.output.value.plan.destination` let you extract data from anywhere in your outputs\n",
    "\n",
    "2. **Multiple Check Types**: \n",
    "   - `exact_match`: For precise comparisons\n",
    "   - `contains`: For substring/phrase checking\n",
    "   - `regex`: For pattern matching\n",
    "   - `threshold`: For numeric bounds checking\n",
    "\n",
    "3. **Comprehensive Results**: Every evaluation provides detailed results with:\n",
    "   - What was compared (`resolved_arguments`)\n",
    "   - Pass/fail status for each check\n",
    "   - Aggregate statistics\n",
    "   - Error handling\n",
    "\n",
    "4. **Vendor Neutrality**: FEP works with any system that produces outputs - LLMs, APIs, traditional software, etc.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Check out the advanced examples for YAML-based test case management\n",
    "- Explore semantic similarity and LLM judge checks for more sophisticated evaluations\n",
    "- Learn about batch evaluation and result analysis techniques\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
